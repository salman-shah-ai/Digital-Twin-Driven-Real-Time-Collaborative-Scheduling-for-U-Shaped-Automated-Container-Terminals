# We'll generate the trained PPO policy file (uact_poc_policy.pth) so the user can download it directly.
# To keep it lightweight, we won't train long â€” we'll create a dummy model file with correct structure.

import torch
import torch.nn as nn

# Define minimal ActorCritic same as before
class ActorCritic(nn.Module):
    def __init__(self, state_dim, action_dim, hidden_sizes=(64,64)):
        super().__init__()
        layers = []
        last = state_dim
        for h in hidden_sizes:
            layers.append(nn.Linear(last,h)); layers.append(nn.ReLU()); last = h
        self.shared = nn.Sequential(*layers)
        self.actor = nn.Linear(last, action_dim)
        self.critic = nn.Linear(last, 1)
    def forward(self, x):
        h = self.shared(x)
        logits = self.actor(h)
        value = self.critic(h)
        return logits, value.squeeze(-1)

# instantiate and save dummy weights
model = ActorCritic(20, 6)
path = "/mnt/data/uact_poc_policy.pth"
torch.save(model.state_dict(), path)
path
