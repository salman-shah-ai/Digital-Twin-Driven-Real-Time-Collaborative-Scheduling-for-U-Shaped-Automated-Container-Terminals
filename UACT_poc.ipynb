{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# UACT POC Notebook\n\nThis notebook contains the POC script. Run the code cell below to execute the demo."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\n# uact_poc.py\n# Simplified U-ACT simulator and compact PPO (PyTorch).\nimport math, random, time\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom collections import deque, namedtuple\n\nTask = namedtuple('Task', ['id','type','remaining_proc','yard_slot'])\n\nclass SimpleUACTEnv:\n    def __init__(self, n_containers=40, n_agvs=5, seed=0, et_delay_rate=0.1):\n        random.seed(seed); np.random.seed(seed)\n        self.n_containers = n_containers\n        self.n_agvs = n_agvs\n        self.et_delay_rate = et_delay_rate\n        self.reset()\n\n    def reset(self):\n        n_import = self.n_containers // 2\n        n_export = self.n_containers - n_import\n        self.time = 0.0\n        self.done = False\n        self.tasks_import_pool = deque()\n        self.tasks_export_pool = deque()\n        tid = 0\n        for i in range(n_import):\n            self.tasks_import_pool.append(Task(tid, 'import', remaining_proc=3.0, yard_slot=i))\n            tid += 1\n        for i in range(n_export):\n            self.tasks_export_pool.append(Task(tid, 'export', remaining_proc=3.0, yard_slot=n_import+i))\n            tid += 1\n        self.agvs = [{'id':i,'status':'idle','available_at':0.0,'load':0} for i in range(self.n_agvs)]\n        self.ycs = [{'id':0,'busy_until':0.0,'queue_agv':deque(), 'queue_et':deque(), 'current_task': None}]\n        self.et_schedule = []\n        base_interval = 5.0\n        t = 10.0\n        for i in range(len(self.tasks_import_pool)):\n            delay = np.random.exponential(scale=base_interval) if random.random() < self.et_delay_rate else np.random.uniform(0, base_interval)\n            t += delay\n            self.et_schedule.append({'task_id': i, 'arrival': t, 'served': False})\n        self.agv_waiting_time = {i:0.0 for i in range(self.n_agvs)}\n        self.makespan = 0.0\n        self.completed = []\n        self.transport_pool = deque()\n        for tsk in list(self.tasks_import_pool):\n            self.transport_pool.append(tsk)\n        for tsk in list(self.tasks_export_pool):\n            self.transport_pool.append(tsk)\n        return self._get_state()\n\n    def step_physical(self, dt=1.0):\n        self.time += dt\n        for yc in self.ycs:\n            if yc['current_task'] is not None and yc['busy_until'] <= self.time:\n                task = yc['current_task']\n                yc['current_task'] = None\n                self.completed.append((task.id, self.time))\n                yc['busy_until'] = 0.0\n        for et in self.et_schedule:\n            if not et['served'] and et['arrival'] <= self.time:\n                yc = self.ycs[0]\n                yc['queue_et'].append(et['task_id'])\n                et['served'] = True\n\n    def scheduling_point(self):\n        idle_agvs = [agv for agv in self.agvs if agv['status']=='idle']\n        return len(self.transport_pool)>0 and len(idle_agvs)>0\n\n    def apply_action(self, action_rule_idx):\n        task_rule = 'LRTC' if action_rule_idx in [0,2,4] else 'SRTC'\n        agv_rule = 'EUTA' if action_rule_idx in [0,1] else ('SPTA' if action_rule_idx in [2,3] else 'NLRA')\n        yc = self.ycs[0]\n        et_waiting = len(yc['queue_et'])>0\n        tasks = list(self.transport_pool)\n        if len(tasks)==0:\n            return 0\n        reverse = True if task_rule=='LRTC' else False\n        tasks_sorted = sorted(tasks, key=lambda t: t.remaining_proc, reverse=reverse)\n        if et_waiting:\n            tasks_sorted = sorted(tasks_sorted, key=lambda t: 0 if t.type=='export' else 1)\n        idle_agvs = [agv for agv in self.agvs if agv['status']=='idle']\n        if len(idle_agvs)==0:\n            return 0\n        if agv_rule=='EUTA':\n            agv_order = idle_agvs\n            random.shuffle(agv_order)\n        else:\n            agv_order = sorted(idle_agvs, key=lambda a: a['load'])\n        assigned = 0\n        for agv in agv_order:\n            if len(tasks_sorted)==0: break\n            task = tasks_sorted.pop(0)\n            travel_time = 2.0 + 0.5*task.remaining_proc\n            agv['status']='busy'\n            agv['available_at'] = self.time + travel_time\n            agv['load'] += 1\n            assigned += 1\n            for i,t in enumerate(self.transport_pool):\n                if t.id==task.id:\n                    self.transport_pool.remove(t)\n                    break\n            self.ycs[0]['queue_agv'].append(task.id)\n        return assigned\n\n    def advance_to_next_event(self):\n        times = []\n        for agv in self.agvs:\n            if agv['status']=='busy':\n                times.append(agv['available_at'])\n                if agv['available_at'] <= self.time:\n                    agv['status']='idle'\n        for et in self.et_schedule:\n            if not et['served']:\n                times.append(et['arrival'])\n        yc = self.ycs[0]\n        if yc['current_task'] is None and len(yc['queue_agv'])>0:\n            task_id = yc['queue_agv'].popleft()\n            yc['current_task'] = Task(task_id, 'unknown', remaining_proc=0.0, yard_slot=0)\n            service_time = 1.0 + random.random()*1.0\n            yc['busy_until'] = self.time + service_time\n            times.append(yc['busy_until'])\n        if len(times)==0:\n            self.done = True\n            return\n        next_t = min(times)\n        dt = max(0.01, next_t - self.time)\n        self.step_physical(dt)\n\n    def get_metrics(self):\n        return {'time': self.time, 'completed': len(self.completed)}\n\n    def _get_state(self):\n        n_import = sum(1 for t in self.transport_pool if t.type=='import')\n        n_export = sum(1 for t in self.transport_pool if t.type=='export')\n        imports = [t.remaining_proc for t in self.transport_pool if t.type=='import']\n        exports = [t.remaining_proc for t in self.transport_pool if t.type=='export']\n        def mean_std(arr):\n            if len(arr)==0:\n                return 0.0, 0.0\n            a = np.array(arr)\n            return float(a.mean()), float(a.std())\n        cr_i_mean, cr_i_std = mean_std(imports)\n        cr_o_mean, cr_o_std = mean_std(exports)\n        all_rem = [t.remaining_proc for t in self.transport_pool]\n        rem_mean, rem_std = mean_std(all_rem)\n        agv_loads = [agv['load'] for agv in self.agvs]\n        agv_mean, agv_std = mean_std(agv_loads)\n        yc_load = len(self.ycs[0]['queue_agv']) + len(self.ycs[0]['queue_et'])\n        yc_mean, yc_std = float(yc_load), 0.0\n        qlen_agv = len(self.ycs[0]['queue_agv'])\n        qlen_agv_std = 0.0\n        agv_wait_mean = np.mean([max(0.0, self.time - agv['available_at']) if agv['status']=='busy' else 0.0 for agv in self.agvs]) if len(self.agvs)>0 else 0.0\n        agv_wait_std = 0.0\n        qlen_et = len(self.ycs[0]['queue_et'])\n        qlen_et_std = 0.0\n        et_wait_mean = 0.0\n        et_wait_std = 0.0\n        state = np.array([\n            n_import, n_export,\n            cr_i_mean, cr_i_std,\n            cr_o_mean, cr_o_std,\n            rem_mean, rem_std,\n            agv_mean, agv_std,\n            yc_mean, yc_std,\n            qlen_agv, qlen_agv_std,\n            agv_wait_mean, agv_wait_std,\n            qlen_et, qlen_et_std,\n            et_wait_mean, et_wait_std\n        ], dtype=np.float32)\n        return state\n\n# PPO components\nclass ActorCritic(nn.Module):\n    def __init__(self, state_dim, action_dim, hidden_sizes=(64,64)):\n        super().__init__()\n        layers = []\n        last = state_dim\n        for h in hidden_sizes:\n            layers.append(nn.Linear(last,h)); layers.append(nn.ReLU()); last = h\n        self.shared = nn.Sequential(*layers)\n        self.actor = nn.Linear(last, action_dim)\n        self.critic = nn.Linear(last, 1)\n    def forward(self, x):\n        h = self.shared(x)\n        logits = self.actor(h)\n        value = self.critic(h)\n        return logits, value.squeeze(-1)\n\ndef entropy_from_logits(logits):\n    a = F.softmax(logits, dim=-1)\n    return -(a * F.log_softmax(logits, dim=-1)).sum(-1)\n\ndef compute_reward(env_before, env_after, alpha=0.6):\n    N = env_before.n_containers\n    before_completed = len(env_before.completed)\n    after_completed = len(env_after.completed)\n    delta_cr = (after_completed - before_completed) / max(1, N)\n    before_queue = len(env_before.ycs[0]['queue_agv'])\n    after_queue = len(env_after.ycs[0]['queue_agv'])\n    delta_queue = max(0, after_queue - before_queue)\n    reward = alpha * delta_cr - (1-alpha) * (delta_queue / max(1, env_before.n_agvs))\n    return reward\n\ndef train_one_episode(env, model, optim, max_steps=200, gamma=0.95, clip=0.2):\n    states, actions, old_logps, rewards, values = [], [], [], [], []\n    s = env.reset()\n    total_reward = 0.0\n    for step in range(max_steps):\n        if env.scheduling_point():\n            state = env._get_state()\n            logits, value = model(torch.tensor(state, dtype=torch.float32).unsqueeze(0))\n            probs = F.softmax(logits, dim=-1)\n            dist = torch.distributions.Categorical(probs)\n            a = int(dist.sample().item())\n            logp = dist.log_prob(torch.tensor(a)).item()\n            env_copy = env\n            env.apply_action(a)\n            env.advance_to_next_event()\n            r = compute_reward(env_copy, env, alpha=0.6)\n            states.append(state); actions.append(a); old_logps.append(logp); rewards.append(r); values.append(float(value.item()))\n            total_reward += r\n        else:\n            env.advance_to_next_event()\n        if env.done or len(env.completed)>=env.n_containers:\n            break\n    if len(states)==0:\n        return total_reward\n    returns = []\n    G=0.0\n    for r in reversed(rewards):\n        G = r + gamma*G\n        returns.insert(0,G)\n    returns = torch.tensor(returns, dtype=torch.float32)\n    returns = (returns - returns.mean())/(returns.std()+1e-8)\n    states_t = torch.tensor(np.array(states), dtype=torch.float32)\n    actions_t = torch.tensor(actions, dtype=torch.long)\n    old_logps_t = torch.tensor(old_logps, dtype=torch.float32)\n    for epoch in range(4):\n        logits, values_pred = model(states_t)\n        dist = torch.distributions.Categorical(F.softmax(logits, dim=-1))\n        new_logps = dist.log_prob(actions_t)\n        ratio = torch.exp(new_logps - old_logps_t)\n        adv = returns - values_pred.detach()\n        surr1 = ratio * adv\n        surr2 = torch.clamp(ratio, 1.0-clip, 1.0+clip) * adv\n        policy_loss = -torch.min(surr1, surr2).mean()\n        value_loss = F.mse_loss(values_pred, returns)\n        ent = entropy_from_logits(logits).mean()\n        loss = policy_loss + 0.5*value_loss - 0.01*ent\n        optim.zero_grad(); loss.backward(); optim.step()\n    return total_reward\n\n# Demo run (short)\nif __name__ == '__main__':\n    state_dim = 20; action_dim = 6\n    model = ActorCritic(state_dim, action_dim)\n    optim = torch.optim.Adam(model.parameters(), lr=3e-4)\n    env = SimpleUACTEnv(n_containers=40, n_agvs=5, seed=2)\n    episodes = 20\n    for ep in range(episodes):\n        r = train_one_episode(env, model, optim, max_steps=300)\n        if (ep+1)%5==0:\n            print(f'Ep {ep+1}/{episodes}, reward={r:.4f}, time={env.time:.2f}, completed={len(env.completed)}')\n    torch.save(model.state_dict(), '/mnt/data/uact_poc_policy.pth')\n    print('Saved demo policy to /mnt/data/uact_poc_policy.pth')\n"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python"}}, "nbformat": 4, "nbformat_minor": 5}